{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: Urdu Deepfake Audio Detection (Binary Classification)"
      ],
      "metadata": {
        "id": "lt0BhwXzarCw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_P5xq3vjsJD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import librosa\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression, Perceptron\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "import joblib\n",
        "\n",
        "# ==========================================\n",
        "# Step 1: Preprocessing & Feature Extraction\n",
        "# ==========================================\n",
        "\n",
        "# Load the Dataset\n",
        "labels_df = pd.read_csv(\"labels.csv\")  # Columns: [audio_filename, label (0=Deepfake, 1=Bonafide]\n",
        "audio_paths = labels_df[\"audio_filename\"].values\n",
        "y = labels_df[\"label\"].values\n",
        "\n",
        "# Extract MFCC Features\n",
        "def extract_mfcc(audio_path, n_mfcc=13, fixed_length=2):\n",
        "    y, sr = librosa.load(audio_path, sr=22050, duration=fixed_length)  # Force 2-second clips\n",
        "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
        "\n",
        "    # Pad/Crop to fixed length (e.g., 2 seconds)\n",
        "    if mfcc.shape[1] < 87:  # 87 frames â‰ˆ 2 seconds for 22050 Hz\n",
        "        mfcc = np.pad(mfcc, ((0, 0), (0, 87 - mfcc.shape[1])))\n",
        "    else:\n",
        "        mfcc = mfcc[:, :87]\n",
        "\n",
        "    return mfcc.flatten()  # Convert to 1D vector (e.g., 13 MFCCs * 87 frames = 1131 features)\n",
        "\n",
        "# Extract features for all audio files\n",
        "X = np.array([extract_mfcc(f\"audio_files/{path}\") for path in audio_paths])\n",
        "\n",
        "# Normalize Features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# =============================\n",
        "# Step 2: Model Building\n",
        "# =============================\n",
        "\n",
        "# Scikit-Learn Models (SVM, Logistic Regression, Perceptron)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "# Train models\n",
        "svm = SVC(kernel=\"rbf\", probability=True).fit(X_train, y_train)\n",
        "lr = LogisticRegression().fit(X_train, y_train)\n",
        "perceptron = Perceptron().fit(X_train, y_train)\n",
        "\n",
        "# PyTorch DNN\n",
        "class AudioDNN(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "# Initialize\n",
        "model = AudioDNN(X_train.shape[1])\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "X_train_tensor = torch.FloatTensor(X_train)\n",
        "y_train_tensor = torch.FloatTensor(y_train).unsqueeze(1)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(20):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# =====================\n",
        "# Step 3: Evaluation\n",
        "# =====================\n",
        "\n",
        "# Scikit-Learn Models\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
        "\n",
        "    metrics = {\n",
        "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
        "        \"Precision\": precision_score(y_test, y_pred),\n",
        "        \"Recall\": recall_score(y_test, y_pred),\n",
        "        \"F1-Score\": f1_score(y_test, y_pred),\n",
        "        \"AUC-ROC\": roc_auc_score(y_test, y_proba) if y_proba is not None else \"N/A\"\n",
        "    }\n",
        "    return metrics\n",
        "\n",
        "print(\"SVM:\", evaluate_model(svm, X_test, y_test))\n",
        "print(\"Logistic Regression:\", evaluate_model(lr, X_test, y_test))\n",
        "print(\"Perceptron:\", evaluate_model(perceptron, X_test, y_test))\n",
        "\n",
        "# PyTorch DNN\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    y_proba = model(torch.FloatTensor(X_test)).numpy().flatten()\n",
        "    y_pred = (y_proba > 0.5).astype(int)\n",
        "\n",
        "print(\"DNN Metrics:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"AUC-ROC:\", roc_auc_score(y_test, y_proba))\n",
        "\n",
        "# =====================\n",
        "# Step 4: Save Models\n",
        "# =====================\n",
        "\n",
        "# Save scikit-learn models and scaler\n",
        "joblib.dump(svm, \"svm_model.pkl\")\n",
        "joblib.dump(lr, \"logistic_model.pkl\")\n",
        "joblib.dump(perceptron, \"perceptron_model.pkl\")\n",
        "joblib.dump(scaler, \"scaler.pkl\")\n",
        "\n",
        "# Save PyTorch model\n",
        "torch.save(model.state_dict(), \"dnn_model.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2: Multi-Label Defect Prediction (Multi-Label Classification)"
      ],
      "metadata": {
        "id": "05yqWPO_aLJU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import hamming_loss, f1_score\n",
        "from sklearn.linear_model import LogisticRegression, Perceptron\n",
        "from sklearn.svm import LinearSVC\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import joblib\n",
        "\n",
        "# =============================\n",
        "# Step 1: Data Preprocessing\n",
        "# =============================\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"dataset.csv\")\n",
        "texts = df[\"report\"].values\n",
        "labels = df.drop(\"report\", axis=1).values  # Shape: (n_samples, n_labels)\n",
        "\n",
        "# Check label distribution\n",
        "print(\"\\nLabel Distribution:\")\n",
        "print(df.drop(\"report\", axis=1).sum(axis=0))\n",
        "\n",
        "# TF-IDF Vectorization\n",
        "tfidf = TfidfVectorizer(max_features=5000, stop_words=\"english\")\n",
        "X = tfidf.fit_transform(texts)\n",
        "\n",
        "# Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# =============================================\n",
        "# Step 2: Model Building & Training\n",
        "# =============================================\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.X.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (\n",
        "            torch.FloatTensor(self.X[idx].toarray()).squeeze(),\n",
        "            torch.FloatTensor(self.y[idx])\n",
        "        )\n",
        "\n",
        "class DNN(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, output_dim),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "def train_pytorch_model():\n",
        "    # Initialize model\n",
        "    model = DNN(X_train.shape[1], y_train.shape[1])\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "    # Create DataLoader\n",
        "    train_loader = DataLoader(\n",
        "        TextDataset(X_train, y_train),\n",
        "        batch_size=32,\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(15):\n",
        "        for batch_x, batch_y in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_x)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    return model\n",
        "\n",
        "# Train all models\n",
        "print(\"\\nTraining Models...\")\n",
        "\n",
        "# 1. Logistic Regression (One-vs-Rest)\n",
        "lr_model = LogisticRegression(max_iter=1000, class_weight=\"balanced\")\n",
        "lr_model = OneVsRestClassifier(lr_model).fit(X_train, y_train)\n",
        "\n",
        "# 2. SVM (One-vs-Rest)\n",
        "svm_model = LinearSVC(class_weight=\"balanced\", dual=False)\n",
        "svm_model = OneVsRestClassifier(svm_model).fit(X_train, y_train)\n",
        "\n",
        "# 3. Online Perceptron\n",
        "perceptron = Perceptron()\n",
        "perceptron = OneVsRestClassifier(perceptron)\n",
        "for i in range(X_train.shape[0]):\n",
        "    perceptron.partial_fit(X_train[i], y_train[i], classes=range(y_train.shape[1]))\n",
        "\n",
        "# 4. PyTorch DNN\n",
        "dnn_model = train_pytorch_model()\n",
        "\n",
        "# =============================================\n",
        "# Step 3: Evaluation\n",
        "# =============================================\n",
        "\n",
        "def evaluate(y_true, y_pred, model_name):\n",
        "    return {\n",
        "        \"Model\": model_name,\n",
        "        \"Hamming Loss\": hamming_loss(y_true, y_pred),\n",
        "        \"Micro-F1\": f1_score(y_true, y_pred, average=\"micro\"),\n",
        "        \"Macro-F1\": f1_score(y_true, y_pred, average=\"macro\")\n",
        "    }\n",
        "\n",
        "# Generate predictions\n",
        "results = []\n",
        "\n",
        "# Scikit-learn models\n",
        "for name, model in [(\"Logistic Regression\", lr_model),\n",
        "                    (\"SVM\", svm_model),\n",
        "                    (\"Perceptron\", perceptron)]:\n",
        "    y_pred = model.predict(X_test)\n",
        "    results.append(evaluate(y_test, y_pred, name))\n",
        "\n",
        "# PyTorch DNN\n",
        "dnn_model.eval()\n",
        "with torch.no_grad():\n",
        "    y_proba = dnn_model(torch.FloatTensor(X_test.toarray())).numpy()\n",
        "y_pred_dnn = (y_proba > 0.5).astype(int)\n",
        "results.append(evaluate(y_test, y_pred_dnn, \"PyTorch DNN\"))\n",
        "\n",
        "# Display results\n",
        "print(\"\\nEvaluation Results:\")\n",
        "print(pd.DataFrame(results))\n",
        "\n",
        "# =============================================\n",
        "# Step 4: Save Artifacts\n",
        "# =============================================\n",
        "\n",
        "# Save models and vectorizer\n",
        "joblib.dump(lr_model, \"logistic_regression_model.pkl\")\n",
        "joblib.dump(svm_model, \"svm_model.pkl\")\n",
        "joblib.dump(perceptron, \"perceptron_model.pkl\")\n",
        "joblib.dump(tfidf, \"tfidf_vectorizer.pkl\")\n",
        "torch.save(dnn_model.state_dict(), \"dnn_model.pth\")\n",
        "\n",
        "# =============================================\n",
        "# Step 5: Streamlit App Integration\n",
        "# =============================================\n",
        "\n",
        "def streamlit_app():\n",
        "    import streamlit as st\n",
        "\n",
        "    st.title(\"Software Defect Prediction\")\n",
        "\n",
        "    # Load artifacts\n",
        "    tfidf = joblib.load(\"tfidf_vectorizer.pkl\")\n",
        "    lr_model = joblib.load(\"logistic_regression_model.pkl\")\n",
        "    dnn_model = DNN(X_train.shape[1], y_train.shape[1])\n",
        "    dnn_model.load_state_dict(torch.load(\"dnn_model.pth\"))\n",
        "    dnn_model.eval()\n",
        "\n",
        "    # UI Components\n",
        "    model_choice = st.selectbox(\"Select Model\", [\"Logistic Regression\", \"Perceptron\", \"DNN\"])\n",
        "    user_input = st.text_area(\"Enter software defect report:\")\n",
        "\n",
        "    if st.button(\"Predict\"):\n",
        "        # Preprocess input\n",
        "        X_input = tfidf.transform([user_input])\n",
        "\n",
        "        # Get predictions\n",
        "        if model_choice == \"Logistic Regression\":\n",
        "            preds = lr_model.predict(X_input)[0]\n",
        "        elif model_choice == \"Perceptron\":\n",
        "            preds = perceptron.predict(X_input)[0]\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                preds = (dnn_model(torch.FloatTensor(X_input.toarray())) > 0.5).float().numpy()[0]\n",
        "\n",
        "        # Display results\n",
        "        labels = df.columns[1:]\n",
        "        results = [labels[i] for i, val in enumerate(preds) if val == 1]\n",
        "        st.write(\"Predicted Defects:\", \", \".join(results) if results else \"No defects predicted\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    streamlit_app()"
      ],
      "metadata": {
        "id": "qbw0nD4raN8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3: Interactive Streamlit App"
      ],
      "metadata": {
        "id": "9w7P9JIWazk3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import librosa\n",
        "import numpy as np\n",
        "import joblib\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# ======================\n",
        "# Model Loading\n",
        "# ======================\n",
        "audio_scaler = joblib.load(\"scaler.pkl\")\n",
        "audio_svm = joblib.load(\"svm_model.pkl\")\n",
        "audio_lr = joblib.load(\"lr_model.pkl\")\n",
        "text_tfidf = joblib.load(\"tfidf.pkl\")\n",
        "text_lr = joblib.load(\"lr_model.pkl\")\n",
        "text_perceptron = joblib.load(\"perceptron_model.pkl\")\n",
        "\n",
        "# ======================\n",
        "# Audio Components\n",
        "# ======================\n",
        "class AudioClassifier(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super().__init__():\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "def process_audio(audio_file):\n",
        "    y, sr = librosa.load(audio_file, sr=22050)\n",
        "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
        "    features = np.concatenate([np.mean(mfcc,1), np.std(mfcc,1)])\n",
        "    return audio_scaler.transform([features])[0]\n",
        "\n",
        "# ======================\n",
        "# Text Components\n",
        "# ======================\n",
        "class TextClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(input_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, output_dim),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "# ======================\n",
        "# Streamlit UI\n",
        "# ======================\n",
        "st.set_page_config(layout=\"wide\")\n",
        "\n",
        "# Audio Section\n",
        "with st.expander(\"ðŸ”Š Deepfake Audio Detection\", expanded=True):\n",
        "    audio_file = st.file_uploader(\"Upload audio\", type=[\"wav\", \"mp3\"])\n",
        "    audio_model = st.selectbox(\"Audio Model\", [\"SVM\", \"Logistic Regression\", \"DNN\"])\n",
        "\n",
        "    if audio_file:\n",
        "        features = process_audio(audio_file)\n",
        "        if audio_model == \"DNN\":\n",
        "            model = AudioClassifier(len(features))\n",
        "            model.load_state_dict(torch.load(\"dnn_model.pt\"))\n",
        "            with torch.no_grad():\n",
        "                proba = model(torch.FloatTensor(features)).item()\n",
        "        else:\n",
        "            model = audio_svm if audio_model == \"SVM\" else audio_lr\n",
        "            proba = model.predict_proba([features])[0][1]\n",
        "        st.write(f\"Prediction: {'Real' if proba > 0.5 else 'Fake'} ({proba:.2%})\")\n",
        "\n",
        "# Text Section\n",
        "with st.expander(\"ðŸ“ Defect Prediction\", expanded=True):\n",
        "    text_input = st.text_area(\"Input defect report\")\n",
        "    text_model = st.selectbox(\"Text Model\", [\"Logistic Regression\", \"Perceptron\", \"DNN\"])\n",
        "\n",
        "    if text_input:\n",
        "        X = text_tfidf.transform([text_input])\n",
        "        if text_model == \"DNN\":\n",
        "            model = TextClassifier(X.shape[1], text_lr.classes_.shape[0])\n",
        "            model.load_state_dict(torch.load(\"dnn_model.pt\"))\n",
        "            with torch.no_grad():\n",
        "                preds = model(torch.FloatTensor(X.toarray())).numpy()[0]\n",
        "        else:\n",
        "            model = text_lr if text_model == \"Logistic Regression\" else text_perceptron\n",
        "            preds = model.predict_proba(X)[0]\n",
        "        labels = [text_lr.classes_[i] for i in np.where(preds > 0.5)[0]]\n",
        "        st.write(\"Predicted defects:\", \", \".join(labels) if labels else \"None\")"
      ],
      "metadata": {
        "id": "4OVHrvM5az34"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}